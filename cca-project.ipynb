{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6533284,"sourceType":"datasetVersion","datasetId":3777053},{"sourceId":7027737,"sourceType":"datasetVersion","datasetId":4041962},{"sourceId":7027842,"sourceType":"datasetVersion","datasetId":4042022},{"sourceId":155046525,"sourceType":"kernelVersion"}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samadnajm/cca-project?scriptVersionId=162609965\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install ml_collections\n!pip install -U neptune-pytorch\n!pip install neptune\n#from multi_scale_attention import DAF_stack\n#from metapolyp import Metapolyp\n#import timm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import neptune\nimport os\nfrom getpass import getpass\nfrom neptune_pytorch import NeptuneLogger\nfrom neptune.utils import stringify_unsupported\nos.environ[\"NEPTUNE_API_TOKEN\"] = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhZjVmMmM4OS1jYjNkLTRlMTAtYjgxYy01MzE0ZjcwMWZiYjgifQ==\"\nos.environ[\"NEPTUNE_PROJECT\"] = \"MedicalImaging/CCA\"\n\nrun = neptune.init_run()","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:16.294691Z","iopub.execute_input":"2024-02-11T10:53:16.295044Z","iopub.status.idle":"2024-02-11T10:53:21.147564Z","shell.execute_reply.started":"2024-02-11T10:53:16.295011Z","shell.execute_reply":"2024-02-11T10:53:21.146673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function, division\nimport albumentations as A\nfrom PIL import ImageEnhance\n\nfrom glob import glob\nimport cv2\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom PIL import Image, ImageOps\nfrom random import random, randint, uniform\nfrom sklearn.preprocessing import MinMaxScaler\nfrom skimage.transform import rotate\nfrom torchvision.transforms import v2\n# Ignore warnings\nimport warnings\nimport pdb\nimport imgaug.augmenters as iaa\n\nwarnings.filterwarnings(\"ignore\")\n#---------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------\ndef create_image_mask_paths(path, image_ext='png', mask_ext='png'):\n    image_paths = glob(os.path.join(path, \"images\", f\"*.{image_ext}\"))\n    mask_paths = glob(os.path.join(path, \"masks\", f\"*.{mask_ext}\"))\n\n    image_ids = [image_path.split(\"_\")[1] for image_path in image_paths]\n    mask_ids = [mask_path.split(\"_\")[1] for mask_path in mask_paths]\n\n    img_dict = {'train': [], 'test': [], 'val': []}\n    mask_dict = {'train': [], 'test': [], 'val': []}\n\n    val_ids = [8, 9]\n    test_ids = [10, 11]\n\n    for image_path, image_id, mask_path, mask_id in zip(image_paths, image_ids, mask_paths, mask_ids):\n        image_id = int(image_id)\n        mask_id = int(mask_id)\n\n        if image_id in test_ids and mask_id in test_ids:\n            img_dict['test'].append(image_path)\n            mask_dict['test'].append(mask_path)\n\n        elif image_id in val_ids and mask_id in val_ids:\n            img_dict['val'].append(image_path)\n            mask_dict['val'].append(mask_path)\n        else:\n            img_dict['train'].append(image_path)\n            mask_dict['train'].append(mask_path)\n\n    return img_dict, mask_dict\n\n#---------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------\ndef make_dataset(root, mode):\n    assert mode in ['train', 'val', 'test']\n    image_dict, mask_dict = create_image_mask_paths(root)\n    items = []\n    if mode == 'train':\n        images = image_dict['train']\n        labels = mask_dict['train']\n\n        images.sort()\n        labels.sort()\n        \n        for it_im, it_gt in zip(images, labels):\n            item = it_im, it_gt\n            items.append(item)\n    elif mode == 'val':\n        images = image_dict['val']\n        labels = mask_dict['val']\n\n        images.sort()\n        labels.sort()\n\n        for it_im, it_gt in zip(images, labels):\n            item = it_im, it_gt\n            items.append(item)\n    else:\n        images = image_dict['test']\n        labels = mask_dict['test']\n\n        images.sort()\n        labels.sort()\n\n        for it_im, it_gt in zip(images, labels):\n            item = it_im, it_gt\n            items.append(item)\n\n    return items\n\n\nclass MedicalImageDataset(Dataset):\n    \"\"\"CHAOS dataset.\"\"\"\n\n    def __init__(self, mode,noise_typ, root_dir = '/kaggle/input/cis-data/data', augment=False, noise=False):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.root_dir = root_dir\n        self.imgs = make_dataset(root_dir, mode)\n        self.augmentation = augment\n        self.Noise = noise\n        self.noise_typ = noise_typ\n        self.scaler = MinMaxScaler()\n\n    def __len__(self):\n        return len(self.imgs)\n    \n    def noisy(self,image,noise_typ):\n       if noise_typ == \"gauss\":\n          row,col,ch= image.shape\n          mean = 0\n          var = 0.02\n          sigma = var**0.5\n          gauss = np.random.normal(mean,sigma,(row,col,ch))\n          gauss = gauss.reshape(row,col,ch)\n          noisy = image + gauss\n          return noisy\n       elif noise_typ ==\"speckle\":\n          row,col,ch = image.shape\n          gauss = np.random.randn(row,col,ch)\n          gauss = gauss.reshape(row,col,ch)        \n          noisy = image + image * gauss\n       return noisy\n\n    def __getitem__(self, index):\n        img_path, mask_path = self.imgs[index]\n        #---------------------------------------------------------------------\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) \n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        T = A.Resize(height=256, width=256)\n        resized_image = T(image=img, mask = mask)\n        img = resized_image['image'].reshape(1,256,256)\n        mask = resized_image['mask'].reshape(1,256,256)\n        img = self.scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n        #---------------------------------------------------------------------\n        if self.augmentation:\n            transformed = TRAIN_VAL_TRANSFORM(image=img.astype(np.float32), mask=mask)\n            img = transformed['image']\n            mask = transformed['mask']\n        #---------------------------------------------------------------------\n        if self.Noise:\n            if uniform(0,1) > .72:\n                img = self.noisy(img, self.noise_typ)\n            \n        #---------------------------------------------------------------------\n        img = torch.from_numpy(img.reshape(1,256,256).astype(np.float32))\n        mask = torch.from_numpy((mask.reshape(1,256,256)/255).astype(np.float32))\n        #---------------------------------------------------------------------\n      \n        return [img, mask]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-11T10:53:21.861618Z","iopub.execute_input":"2024-02-11T10:53:21.861984Z","iopub.status.idle":"2024-02-11T10:53:24.258986Z","shell.execute_reply.started":"2024-02-11T10:53:21.861952Z","shell.execute_reply":"2024-02-11T10:53:24.257954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the transformations\nTRAIN_VAL_TRANSFORM = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.Rotate(limit=30, p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.RandomGamma(p=0.3),  # Randomly change image gamma\n    A.Blur(p=0.1),  # Apply random blur to the image\n    A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0),  # Apply elastic transformation\n    A.GridDistortion(p=0.5),  # Apply grid distortion\n    A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=.5),  # Apply optical distortion\n])","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:24.260692Z","iopub.execute_input":"2024-02-11T10:53:24.261186Z","iopub.status.idle":"2024-02-11T10:53:24.268172Z","shell.execute_reply.started":"2024-02-11T10:53:24.261159Z","shell.execute_reply":"2024-02-11T10:53:24.2672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**U-NET**","metadata":{}},{"cell_type":"code","source":"\"\"\" Parts of the U-Net model \"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:24.274653Z","iopub.execute_input":"2024-02-11T10:53:24.275238Z","iopub.status.idle":"2024-02-11T10:53:24.292468Z","shell.execute_reply.started":"2024-02-11T10:53:24.275207Z","shell.execute_reply":"2024-02-11T10:53:24.291516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Full assembly of the parts to form the complete network \"\"\"\nclass UNet_base(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=False):\n        super(UNet_base, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = (DoubleConv(n_channels, 64))\n        self.down1 = (Down(64, 128))\n        self.down2 = (Down(128, 256))\n        self.down3 = (Down(256, 512))\n        factor = 2 if bilinear else 1\n        self.down4 = (Down(512, 1024 // factor))\n        self.up1 = (Up(1024, 512 // factor, bilinear))\n        self.up2 = (Up(512, 256 // factor, bilinear))\n        self.up3 = (Up(256, 128 // factor, bilinear))\n        self.up4 = (Up(128, 64, bilinear))\n        self.outc = (OutConv(64, n_classes))\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return x1, x,  logits\n\n    def use_checkpointing(self):\n        self.inc = torch.utils.checkpoint(self.inc)\n        self.down1 = torch.utils.checkpoint(self.down1)\n        self.down2 = torch.utils.checkpoint(self.down2)\n        self.down3 = torch.utils.checkpoint(self.down3)\n        self.down4 = torch.utils.checkpoint(self.down4)\n        self.up1 = torch.utils.checkpoint(self.up1)\n        self.up2 = torch.utils.checkpoint(self.up2)\n        self.up3 = torch.utils.checkpoint(self.up3)\n        self.up4 = torch.utils.checkpoint(self.up4)\n        self.outc = torch.utils.checkpoint(self.outc)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:24.294428Z","iopub.execute_input":"2024-02-11T10:53:24.294738Z","iopub.status.idle":"2024-02-11T10:53:24.309341Z","shell.execute_reply.started":"2024-02-11T10:53:24.294713Z","shell.execute_reply":"2024-02-11T10:53:24.308395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ViT Model**","metadata":{}},{"cell_type":"code","source":"import ml_collections\n\ndef get_b16_config():\n    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 768\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 3072\n    config.transformer.num_heads = 12\n    config.transformer.num_layers = 12\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n\n    config.classifier = 'seg'\n    config.representation_size = None\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '../model/vit_checkpoint/imagenet21k/ViT-B_16.npz'\n    config.patch_size = 16\n\n    config.decoder_channels = (256, 128, 64, 16)\n    config.n_classes = 1\n    config.activation = 'softmax'\n    \n    config.n_skip = 0\n    return config\n\n\ndef get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\ndef get_r50_b16_config():\n    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.pretrained_path = '../model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 1\n    config.n_skip = 0\n    config.activation = 'softmax'\n\n    return config\n\n\ndef get_b32_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.grid = None\n    config.patches.size = (32, 32)\n    config.patch_size = 32\n    config.pretrained_path = '../model/vit_checkpoint/imagenet21k/ViT-B_32.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    \n    config.n_skip = 0\n    return config\n\n\ndef get_l16_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1024\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 4096\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 24\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.representation_size = None\n\n    # custom\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = None\n    config.pretrained_path = '../model/vit_checkpoint/imagenet21k/ViT-L_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 1\n    config.activation = 'softmax'\n    \n    config.n_skip = 0\n    return config\n\n\ndef get_r50_l16_config():\n    \"\"\"Returns the Resnet50 + ViT-L/16 configuration. customized \"\"\"\n    config = get_l16_config()\n    config.patches.grid = (16, 16)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n\n    config.classifier = 'seg'\n    config.resnet_pretrained_path = '../model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz'\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_classes = 1\n    config.activation = 'softmax'\n    \n    config.n_skip = 0\n    return config\n\n\ndef get_l32_config():\n    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n    config = get_l16_config()\n    config.patches.grid = None\n    config.patches.size = (32, 32)\n    config.patch_size = 32\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    \n    config.n_skip = 0\n    return config\n\n\ndef get_h14_config(): # decoded_channels missing\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n    config.patch_size = 14\n    config.hidden_size = 1280\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 5120\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 32\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    config.decoder_channels = (256, 128, 64, 16)\n    config.skip_channels = [512, 256, 64, 16]\n    config.n_skip = 0\n    return config","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:24.310702Z","iopub.execute_input":"2024-02-11T10:53:24.311092Z","iopub.status.idle":"2024-02-11T10:53:24.348565Z","shell.execute_reply.started":"2024-02-11T10:53:24.311042Z","shell.execute_reply":"2024-02-11T10:53:24.347841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nfrom os.path import join as pjoin\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef np2th(weights, conv=False):\n    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n    if conv:\n        weights = weights.transpose([3, 2, 0, 1])\n    return torch.from_numpy(weights)\n\n\nclass StdConv2d(nn.Conv2d):\n\n    def forward(self, x):\n        w = self.weight\n        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n        w = (w - m) / torch.sqrt(v + 1e-5)\n        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n                        self.dilation, self.groups)\n\n\ndef conv3x3(cin, cout, stride=1, groups=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n                     padding=1, bias=bias, groups=groups)\n\n\ndef conv1x1(cin, cout, stride=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n                     padding=0, bias=bias)\n\n\nclass PreActBottleneck(nn.Module):\n    \"\"\"Pre-activation (v2) bottleneck block.\n    \"\"\"\n\n    def __init__(self, cin, cout=None, cmid=None, stride=1):\n        super().__init__()\n        cout = cout or cin\n        cmid = cmid or cout//4\n\n        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv1 = conv1x1(cin, cmid, bias=False)\n        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n        self.conv3 = conv1x1(cmid, cout, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n        if (stride != 1 or cin != cout):\n            # Projection also with pre-activation according to paper.\n            self.downsample = conv1x1(cin, cout, stride, bias=False)\n            self.gn_proj = nn.GroupNorm(cout, cout)\n\n    def forward(self, x):\n\n        # Residual branch\n        residual = x\n        if hasattr(self, 'downsample'):\n            residual = self.downsample(x)\n            residual = self.gn_proj(residual)\n\n        # Unit's branch\n        y = self.relu(self.gn1(self.conv1(x)))\n        y = self.relu(self.gn2(self.conv2(y)))\n        y = self.gn3(self.conv3(y))\n\n        y = self.relu(residual + y)\n        return y\n\n    def load_from(self, weights, n_block, n_unit):\n        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n\n        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n\n        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n\n        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n\n        self.conv1.weight.copy_(conv1_weight)\n        self.conv2.weight.copy_(conv2_weight)\n        self.conv3.weight.copy_(conv3_weight)\n\n        self.gn1.weight.copy_(gn1_weight.view(-1))\n        self.gn1.bias.copy_(gn1_bias.view(-1))\n\n        self.gn2.weight.copy_(gn2_weight.view(-1))\n        self.gn2.bias.copy_(gn2_bias.view(-1))\n\n        self.gn3.weight.copy_(gn3_weight.view(-1))\n        self.gn3.bias.copy_(gn3_bias.view(-1))\n\n        if hasattr(self, 'downsample'):\n            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n\n            self.downsample.weight.copy_(proj_conv_weight)\n            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n\nclass ResNetV2(nn.Module):\n    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n\n    def __init__(self, block_units, width_factor):\n        super().__init__()\n        width = int(64 * width_factor)\n        self.width = width\n\n        self.root = nn.Sequential(OrderedDict([\n            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n            ('relu', nn.ReLU(inplace=True)),\n            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n        ]))\n\n        self.body = nn.Sequential(OrderedDict([\n            ('block1', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n                ))),\n            ('block2', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n                ))),\n            ('block3', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n                ))),\n        ]))\n\n    def forward(self, x):\n        features = []\n        b, c, in_size, _ = x.size()\n        x = self.root(x)\n        features.append(x)\n        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)(x)\n        for i in range(len(self.body)-1):\n            x = self.body[i](x)\n            right_size = int(in_size / 4 / (i+1))\n            if x.size()[2] != right_size:\n                pad = right_size - x.size()[2]\n                assert pad < 3 and pad > 0, \"x {} should {}\".format(x.size(), right_size)\n                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n            else:\n                feat = x\n            features.append(feat)\n        x = self.body[-1](x)\n        return x, features[::-1]","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:24.349924Z","iopub.execute_input":"2024-02-11T10:53:24.350202Z","iopub.status.idle":"2024-02-11T10:53:24.388094Z","shell.execute_reply.started":"2024-02-11T10:53:24.350179Z","shell.execute_reply":"2024-02-11T10:53:24.387111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport math\n\nfrom os.path import join as pjoin\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\nfrom torch.nn.modules.utils import _pair\nfrom scipy import ndimage\n\n#from . import trans_unet_configs as configs\n#from .trans_unet_modeling_resnet_skip import ResNetV2\n\nlogger = logging.getLogger(__name__)\n\n\nATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\nATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\nATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\nATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\nFC_0 = \"MlpBlock_3/Dense_0\"\nFC_1 = \"MlpBlock_3/Dense_1\"\nATTENTION_NORM = \"LayerNorm_0\"\nMLP_NORM = \"LayerNorm_2\"\n\ndef np2th(weights, conv=False):\n    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n    if conv:\n        weights = weights.transpose([3, 2, 0, 1])\n    return torch.from_numpy(weights)\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n\n\nclass Attention(nn.Module):\n    def __init__(self, config, vis):\n        super(Attention, self).__init__()\n        self.vis = vis\n        self.num_attention_heads = config.transformer[\"num_heads\"]\n        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = Linear(config.hidden_size, self.all_head_size)\n        self.key = Linear(config.hidden_size, self.all_head_size)\n        self.value = Linear(config.hidden_size, self.all_head_size)\n\n        self.out = Linear(config.hidden_size, config.hidden_size)\n        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n\n        self.softmax = Softmax(dim=-1)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        attention_probs = self.softmax(attention_scores)\n        weights = attention_probs if self.vis else None\n        attention_probs = self.attn_dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        attention_output = self.out(context_layer)\n        attention_output = self.proj_dropout(attention_output)\n        return attention_output, weights\n\n\nclass Mlp(nn.Module):\n    def __init__(self, config):\n        super(Mlp, self).__init__()\n        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n        self.act_fn = ACT2FN[\"gelu\"]\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.normal_(self.fc1.bias, std=1e-6)\n        nn.init.normal_(self.fc2.bias, std=1e-6)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Embeddings(nn.Module):\n    \"\"\"Construct the embeddings from patch, position embeddings.\n    \"\"\"\n    def __init__(self, config, img_size, in_channels):\n        super(Embeddings, self).__init__()\n        self.hybrid = None\n        self.config = config\n        img_size = _pair(img_size)\n\n        if config.patches.get(\"grid\") is not None:   # ResNet\n            grid_size = config.patches[\"grid\"]\n            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n            patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n            n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])  \n            self.hybrid = True\n        else:\n            patch_size = _pair(config.patches[\"size\"])\n            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n            self.hybrid = False\n\n        if self.hybrid:\n            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)\n            in_channels = self.hybrid_model.width * 16\n        self.patch_embeddings = Conv2d(in_channels=in_channels,\n                                       out_channels=config.hidden_size,\n                                       kernel_size=patch_size,\n                                       stride=patch_size)\n        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size))\n\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n\n    def forward(self, x):\n        if self.hybrid:\n            if x.shape[1] == 2:\n                pad = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3],device=x.device)\n                x = torch.cat((x,pad), 1)\n            x, features = self.hybrid_model(x)\n        else:\n            features = None\n        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n        x = x.flatten(2)\n        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n        return embeddings, features\n\n\nclass Block(nn.Module):\n    def __init__(self, config, vis):\n        super(Block, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn = Mlp(config)\n        self.attn = Attention(config, vis)\n\n    def forward(self, x):\n        h = x\n        x = self.attention_norm(x)\n        x, weights = self.attn(x)\n        x = x + h\n\n        h = x\n        x = self.ffn_norm(x)\n        x = self.ffn(x)\n        x = x + h\n        return x, weights\n\n    def load_from(self, weights, n_block):\n        ROOT = f\"Transformer/encoderblock_{n_block}\"\n        with torch.no_grad():\n            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n\n            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n\n            self.attn.query.weight.copy_(query_weight)\n            self.attn.key.weight.copy_(key_weight)\n            self.attn.value.weight.copy_(value_weight)\n            self.attn.out.weight.copy_(out_weight)\n            self.attn.query.bias.copy_(query_bias)\n            self.attn.key.bias.copy_(key_bias)\n            self.attn.value.bias.copy_(value_bias)\n            self.attn.out.bias.copy_(out_bias)\n\n            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n\n            self.ffn.fc1.weight.copy_(mlp_weight_0)\n            self.ffn.fc2.weight.copy_(mlp_weight_1)\n            self.ffn.fc1.bias.copy_(mlp_bias_0)\n            self.ffn.fc2.bias.copy_(mlp_bias_1)\n\n            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n\n\nclass Encoder(nn.Module):\n    def __init__(self, config, vis):\n        super(Encoder, self).__init__()\n        self.vis = vis\n        self.layer = nn.ModuleList()\n        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        for _ in range(config.transformer[\"num_layers\"]):\n            layer = Block(config, vis)\n            self.layer.append(copy.deepcopy(layer))\n\n    def forward(self, hidden_states):\n        attn_weights = []\n        for layer_block in self.layer:\n            hidden_states, weights = layer_block(hidden_states)\n            if self.vis:\n                attn_weights.append(weights)\n        encoded = self.encoder_norm(hidden_states)\n        return encoded, attn_weights\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config, img_size, vis, n_channels):\n        super(Transformer, self).__init__()\n        self.embeddings = Embeddings(config, img_size=img_size, in_channels=n_channels)\n        self.encoder = Encoder(config, vis)\n\n    def forward(self, input_ids):\n        embedding_output, features = self.embeddings(input_ids)\n        encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n        return encoded, attn_weights, features\n\n\nclass Conv2dReLU(nn.Sequential):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=0,\n            stride=1,\n            use_batchnorm=True,\n    ):\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not (use_batchnorm),\n        )\n        relu = nn.ReLU(inplace=True)\n\n        bn = nn.BatchNorm2d(out_channels)\n\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            skip_channels=0,\n            use_batchnorm=True,\n    ):\n        super().__init__()\n        self.conv1 = Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.conv2 = Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, x, skip=None):\n        x = self.up(x)\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass SegmentationHead(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n        super().__init__(conv2d, upsampling)\n\n\nclass DecoderCup(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        head_channels = 512\n        self.conv_more = Conv2dReLU(\n            config.hidden_size,\n            head_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=True,\n        )\n        decoder_channels = config[\"decoder_channels\"]\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        out_channels = decoder_channels\n\n        if self.config[\"n_skip\"] != 0:\n            skip_channels = self.config[\"skip_channels\"]\n            for i in range(4 - self.config[\"n_skip\"]):  # re-select the skip channels according to n_skip\n                skip_channels[3-i]=0\n\n        else:\n            skip_channels=[0,0,0,0]\n\n        blocks = [\n            DecoderBlock(in_ch, out_ch, sk_ch) for in_ch, out_ch, sk_ch in zip(in_channels, out_channels, skip_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, hidden_states, features=None):\n        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n        x = hidden_states.permute(0, 2, 1)\n        x = x.contiguous().view(B, hidden, h, w)\n        x = self.conv_more(x)\n        for i, decoder_block in enumerate(self.blocks):\n            if features is not None:\n                skip = features[i] if (i < self.config[\"n_skip\"]) else None\n            else:\n                skip = None\n            x = decoder_block(x, skip=skip)\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, config, img_size=224, num_classes=21843, n_channels=3, zero_head=False, vis=False):\n        super(VisionTransformer, self).__init__()\n        self.img_size = img_size\n        self.num_classes = num_classes\n        self.n_channels = n_channels\n        self.zero_head = zero_head\n        self.classifier = config.classifier\n        self.transformer = Transformer(config, img_size, vis, n_channels=self.n_channels)\n        self.decoder = DecoderCup(config)\n        self.segmentation_head = SegmentationHead(\n            in_channels=config['decoder_channels'][-1],\n            out_channels=num_classes,\n            kernel_size=3,\n        )\n        self.config = config\n\n    def forward(self, x):\n        if x.size()[1] == 1:\n            x = x.repeat(1,3,1,1)\n        x, attn_weights, features = self.transformer(x)  # (B, n_patch, hidden)\n        x = self.decoder(x, features)\n        logits = self.segmentation_head(x)\n        if x.shape[-1] != self.img_size:\n            x = F.interpolate(x, size=self.img_size, mode='bilinear', align_corners=False)\n            \n        if logits.shape[-1] != self.img_size:\n            logits = F.interpolate(logits, size=self.img_size, mode='bilinear', align_corners=False)\n        return x, logits\n\n    def load_from(self, weights):\n        with torch.no_grad():\n\n            res_weight = weights\n            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n\n            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n\n            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n\n            posemb_new = self.transformer.embeddings.position_embeddings\n            if posemb.size() == posemb_new.size():\n                self.transformer.embeddings.position_embeddings.copy_(posemb)\n            elif posemb.size()[1]-1 == posemb_new.size()[1]:\n                posemb = posemb[:, 1:]\n                self.transformer.embeddings.position_embeddings.copy_(posemb)\n            else:\n                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n                ntok_new = posemb_new.size(1)\n                if self.classifier == \"seg\":\n                    _, posemb_grid = posemb[:, :1], posemb[0, 1:]\n                gs_old = int(np.sqrt(len(posemb_grid)))\n                gs_new = int(np.sqrt(ntok_new))\n                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)  # th2np\n                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n                posemb = posemb_grid\n                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n\n            # Encoder whole\n            for bname, block in self.transformer.encoder.named_children():\n                for uname, unit in block.named_children():\n                    unit.load_from(weights, n_block=uname)\n\n            if self.transformer.embeddings.hybrid:\n                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(res_weight[\"conv_root/kernel\"], conv=True))\n                gn_weight = np2th(res_weight[\"gn_root/scale\"]).view(-1)\n                gn_bias = np2th(res_weight[\"gn_root/bias\"]).view(-1)\n                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n\n                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n                    for uname, unit in block.named_children():\n                        unit.load_from(res_weight, n_block=bname, n_unit=uname)\n\n\nTRANSCONFIG = {\n    'ViT-B_16': get_b16_config(), # n_skip missing\n    'ViT-B_32': get_b32_config(),\n    'ViT-L_16': get_l16_config(),\n    'ViT-L_32': get_l32_config(),\n    'ViT-H_14': get_h14_config(),\n    'R50-ViT-B_16': get_r50_b16_config(),\n    'R50-ViT-L_16': get_r50_l16_config(),\n    'testing': get_testing(),\n}","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:24.389312Z","iopub.execute_input":"2024-02-11T10:53:24.389599Z","iopub.status.idle":"2024-02-11T10:53:24.478464Z","shell.execute_reply.started":"2024-02-11T10:53:24.389564Z","shell.execute_reply":"2024-02-11T10:53:24.477472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **IterNet**","metadata":{}},{"cell_type":"code","source":"\"\"\" Parts of the U-Net model \"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:24.717223Z","iopub.execute_input":"2024-02-11T10:53:24.717535Z","iopub.status.idle":"2024-02-11T10:53:24.733078Z","shell.execute_reply.started":"2024-02-11T10:53:24.717508Z","shell.execute_reply":"2024-02-11T10:53:24.731945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Full assembly of the parts to form the complete network \"\"\"\n\nimport torch.nn.functional as F\nfrom torch.nn import ModuleList\nimport torch\n\n#from .unet_parts import *\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, out_channels=32):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        bilinear = False\n\n        self.inc = DoubleConv(n_channels, out_channels)\n        self.down1 = Down(out_channels, out_channels * 2)\n        self.down2 = Down(out_channels * 2, out_channels * 4)\n        self.down3 = Down(out_channels * 4, out_channels * 8)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(out_channels * 8, out_channels * 16 // factor)\n        self.up1 = Up(out_channels * 16, out_channels * 8 // factor, bilinear)\n        self.up2 = Up(out_channels * 8, out_channels * 4 // factor, bilinear)\n        self.up3 = Up(out_channels * 4, out_channels * 2 // factor, bilinear)\n        self.up4 = Up(out_channels * 2, out_channels, bilinear)\n        self.outc = OutConv(out_channels, 3)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return x1, x, logits\n\n\nclass MiniUNet(nn.Module):\n    def __init__(self, n_channels, n_classes, out_channels=32):\n        super(MiniUNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        bilinear = False\n\n        self.inc = DoubleConv(n_channels, out_channels)\n        self.down1 = Down(out_channels, out_channels*2)\n        self.down2 = Down(out_channels*2, out_channels*4)\n        self.down3 = Down(out_channels*4, out_channels*8)\n        self.up1 = Up(out_channels*8, out_channels*4, bilinear)\n        self.up2 = Up(out_channels*4, out_channels*2, bilinear)\n        self.up3 = Up(out_channels*2, out_channels, bilinear)\n        self.outc = OutConv(out_channels, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x = self.up1(x4, x3)\n        x = self.up2(x, x2)\n        x = self.up3(x, x1)\n        logits = self.outc(x)\n        return x1, x, logits\n\n\nclass Iternet(nn.Module):\n    def __init__(self, n_channels, n_classes, out_channels=8, iterations=2, transformer_config=TRANSCONFIG['ViT-B_16']):\n        super(Iternet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.iterations = iterations\n\n        # define the network UNet layer\n        self.UNet = UNet(n_channels=n_channels,\n                               n_classes=n_classes, out_channels=1)\n        # define the network MiniUNet layers\n        self.VisionTransformer = ModuleList(VisionTransformer(transformer_config, img_size=256, num_classes=1, n_channels=self.n_channels, zero_head=False, vis=False) for i in range(iterations))\n        self.model_miniunet = MiniUNet(\n            n_channels=16, n_classes=n_classes, out_channels=out_channels)\n    def forward(self, x):\n        x1, x2,  logit = self.UNet(x)\n        for i in range(self.iterations):\n            x = torch.cat([x1, x2], dim=1)\n            x2, logits = self.VisionTransformer[i](logit)\n            _, x2, logits  = self.model_miniunet(x2)  \n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:25.182592Z","iopub.execute_input":"2024-02-11T10:53:25.183571Z","iopub.status.idle":"2024-02-11T10:53:25.211571Z","shell.execute_reply.started":"2024-02-11T10:53:25.183526Z","shell.execute_reply":"2024-02-11T10:53:25.210452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import Tensor\n\n\ndef dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n    # Average of Dice coefficient for all batches, or for a single mask\n    assert input.size() == target.size()\n    assert input.dim() == 3 or not reduce_batch_first\n\n    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n\n    inter = 2 * (input * target).sum(dim=sum_dim)\n    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n\n    dice = (inter + epsilon) / (sets_sum + epsilon)\n    return dice.mean()\n\n\ndef multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n    # Average of Dice coefficient for all classes\n    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n\n\ndef dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n    # Dice loss (objective to minimize) between 0 and 1\n    fn = multiclass_dice_coeff if multiclass else dice_coeff\n    return 1 - fn(input, target, reduce_batch_first=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:26.309075Z","iopub.execute_input":"2024-02-11T10:53:26.309681Z","iopub.status.idle":"2024-02-11T10:53:26.319468Z","shell.execute_reply.started":"2024-02-11T10:53:26.309647Z","shell.execute_reply":"2024-02-11T10:53:26.31837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PyTorch\nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        Dice_BCE = BCE + dice_loss\n        \n        return Dice_BCE","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:27.171689Z","iopub.execute_input":"2024-02-11T10:53:27.172347Z","iopub.status.idle":"2024-02-11T10:53:27.17922Z","shell.execute_reply.started":"2024-02-11T10:53:27.172315Z","shell.execute_reply":"2024-02-11T10:53:27.178204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass IoUIndex(nn.Module):\n    def __init__(self, smooth=1):\n        super(IoUIndex, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, inputs, targets):\n        # Flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n\n        # Intersection is equivalent to True Positive count\n        # Union is the mutually inclusive area of all labels & predictions\n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection\n\n        IoU = (intersection + self.smooth) / (union + self.smooth)\n\n        return IoU\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:27.736539Z","iopub.execute_input":"2024-02-11T10:53:27.736904Z","iopub.status.idle":"2024-02-11T10:53:27.744031Z","shell.execute_reply.started":"2024-02-11T10:53:27.736874Z","shell.execute_reply":"2024-02-11T10:53:27.743051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef tversky_coef(y_true, y_pred, alpha=0.7, beta=0.3, smooth=1e-5):\n    \"\"\"\n    Compute the Tversky coefficient between predicted and true binary masks.\n\n    Parameters:\n    - y_true (Tensor): Ground truth binary mask tensor (shape: [batch_size, ...]).\n    - y_pred (Tensor): Predicted binary mask tensor (shape: [batch_size, ...]).\n    - alpha (float): Weight parameter for false positives.\n    - beta (float): Weight parameter for false negatives.\n    - smooth (float): Smoothing factor to avoid division by zero.\n\n    Returns:\n    - tversky (Tensor): Tversky coefficient tensor.\n    \"\"\"\n    assert y_true.size() == y_pred.size(), \"Input shapes must match.\"\n\n    # Flatten the tensors\n    y_true_flat = y_true.view(y_true.size(0), -1)\n    y_pred_flat = y_pred.view(y_pred.size(0), -1)\n\n    # True positives, false positives, and false negatives\n    tp = torch.sum(y_true_flat * y_pred_flat, dim=1)\n    fp = torch.sum((1 - y_true_flat) * y_pred_flat, dim=1)\n    fn = torch.sum(y_true_flat * (1 - y_pred_flat), dim=1)\n\n    # Tversky coefficient\n    tversky = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n\n    return tversky.mean()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:28.386194Z","iopub.execute_input":"2024-02-11T10:53:28.386528Z","iopub.status.idle":"2024-02-11T10:53:28.394299Z","shell.execute_reply.started":"2024-02-11T10:53:28.386504Z","shell.execute_reply":"2024-02-11T10:53:28.393338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport logging\n\n@torch.inference_mode()\ndef evaluate(args, net, dataloader, device, amp, criterion, save_checkpoint=False, dir_checkpoint=None):\n    IoU = IoUIndex()\n    net.eval()\n    num_val_batches = len(dataloader)\n    dice_score, total_iou, total_tversky, total_loss = 0, 0, 0, 0\n    best_loss = float('inf')  # Initialize best_loss to positive infinity\n\n    # iterate over the validation set\n    with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n        with tqdm(total=num_val_batches, desc='Validation round', unit='img', position=0, leave=True, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10}', colour='blue') as pbar:\n            for batch in dataloader:\n                image, mask_true = batch[0], batch[1]\n\n                # move images and labels to correct device and type\n                image = image.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n                mask_true = mask_true.to(device=device, dtype=torch.long)\n\n                # predict the mask\n                mask_pred = net(image)\n\n                if args.classes == 1:\n                    assert mask_true.min() >= 0 and mask_true.max() <= 1, 'True mask indices should be in [0, 1]'\n                    mask_pred = (F.sigmoid(mask_pred) > 0.5).float()\n                    # compute the Dice score\n                    batch_dice = dice_coeff(mask_pred, mask_true, reduce_batch_first=False)\n                    dice_score += batch_dice\n                    # Calculate and accumulate Tversky coefficient\n                    batch_tversky = tversky_coef(mask_pred, mask_true)\n                    total_tversky += batch_tversky\n                    # Calculate and accumulate IoU coefficient\n                    batch_iou = IoU(mask_pred, mask_true)\n                    total_iou += batch_iou\n                    # Compute loss\n                    loss = F.binary_cross_entropy_with_logits(mask_pred, mask_true.float())\n                else:\n                    assert mask_true.min() >= 0 and mask_true.max() < args.classes, 'True mask indices should be in [0, n_classes['\n                    # convert to one-hot format\n                    mask_true = F.one_hot(mask_true, args.classes).permute(0, 3, 1, 2).float()\n                    mask_pred = F.one_hot(mask_pred.argmax(dim=1), args.classes).permute(0, 3, 1, 2).float()\n                    # compute the Dice score, ignoring background\n                    dice_score += multiclass_dice_coeff(mask_pred[:, 1:], mask_true[:, 1:], reduce_batch_first=False)\n                    # Compute loss\n                    loss = criterion(mask_pred.squeeze(1), mask_true.squeeze(1).float())\n\n                total_loss += loss.item()\n                pbar.update(1)\n                pbar.set_postfix(\n                    **{\n                        'Dice': f'{batch_dice:.4f}',\n                        'Tversky': f'{batch_tversky:.4f}',\n                        'IoU': f'{batch_iou:.4f}',\n                        'Loss': f'{loss.item():.4f}',\n                    }\n                )\n\n            # Compute average loss for the entire validation set\n            avg_loss = total_loss / num_val_batches\n\n            # Update best_loss and save checkpoint if a new best loss is found\n            if avg_loss < best_loss and True:\n                #print('loss imporved from(',best_loss,') to ', avg_loss )\n                best_loss = avg_loss\n                Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n                state_dict = net.state_dict()\n                state_dict['mask_values'] = [0, 1]\n                torch.save(state_dict, str(dir_checkpoint / 'best_checkpoint.pth'))\n                logging.info(f'Best checkpoint saved! Loss: {avg_loss:.4f}')\n            else:\n                pass\n                #print('LOSS DID NOT IMPROVE')\n\n    print(f'Dice: {(dice_score / max(num_val_batches, 1)):.4f} -Tversky: {(total_tversky / max(num_val_batches, 1)):.4f} -IoU: {(total_iou / max(num_val_batches, 1)):.4f} -Loss: {(avg_loss):.4f}')\n    net.train()\n    return dice_score / max(num_val_batches, 1)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:28.587826Z","iopub.execute_input":"2024-02-11T10:53:28.588156Z","iopub.status.idle":"2024-02-11T10:53:28.605611Z","shell.execute_reply.started":"2024-02-11T10:53:28.588116Z","shell.execute_reply":"2024-02-11T10:53:28.604651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_funcs = {}\nfor iteration_id in range(3):\n    loss_funcs[f'out1{iteration_id + 1}'] = nn.BCEWithLogitsLoss()\n\nloss_funcs['final_out'] = nn.BCEWithLogitsLoss()\n\n# Define evaluation metrics\nmetrics = {\n    'final_out': ['accuracy']\n}\n\n# Define the training loop\ndef train(model, train_loader, optimizer, loss_funcs, metrics):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = model(data)\n        losses = {name: loss_func(output[name], target[name]) for name, loss_func in loss_funcs.items()}\n        loss = sum(losses.values())\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n            \n# Your model training loop continues here...\n\n# Example usage:\n# train(model, train_loader, optimizer, loss_funcs, metrics)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:28.825191Z","iopub.execute_input":"2024-02-11T10:53:28.825787Z","iopub.status.idle":"2024-02-11T10:53:28.833787Z","shell.execute_reply.started":"2024-02-11T10:53:28.825758Z","shell.execute_reply":"2024-02-11T10:53:28.832802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport logging\nimport os\nimport random\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom pathlib import Path\nfrom torch import optim\nfrom torch.utils.data import DataLoader, random_split\nfrom tqdm import tqdm\nfrom torchmetrics import JaccardIndex\n\n\ndir_img = Path('/kaggle/input/cis-data/data/images')\ndir_mask = Path('/kaggle/input/cis-data/data/masks')\ndir_checkpoint = Path('/kaggle/working/checkpoints/')\n\n\ndef train_model(\n        model,\n        args,\n        device,\n        epochs: int = 5,\n        batch_size: int = 1,\n        learning_rate: float = 1e-5,\n        val_percent: float = 0.1,\n        save_checkpoint: bool = True,\n        img_scale: float = 0.5,\n        amp: bool = False,\n        weight_decay: float = 1e-8,\n        momentum: float = 0.999,\n        gradient_clipping: float = 1.0,\n):\n\n    train_set = MedicalImageDataset('train',\n                  augment=True,\n                  noise=True, noise_typ = 'speckle')\n    val_set   = MedicalImageDataset('val',\n                  augment=False,\n                  noise=False, noise_typ = 'speckle')\n    n_val = int(len(val_set))\n    n_train = int(len(train_set))\n    # 3. Create data loaders\n    loader_args = dict(batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True)\n    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n    num_train_batches = len(train_loader)\n\n    parameters = {\n    \"model_filename\": \"basemodel\",\n    'Epochs':          {epochs},\n    'Batch size':      {batch_size},\n    'Learning rate':   {learning_rate},\n    'Training size':   {n_train},\n    'Validation size': {n_val},\n    'Checkpoints':     {save_checkpoint},\n    'Device':          {device.type},\n    'Images scaling':  {img_scale},\n    'Mixed Precision': {amp}\n        }\n    run[npt_logger.base_namespace][\"hyperparams\"] = stringify_unsupported( \n    parameters\n    )\n    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n    #optimizer = optim.RMSprop(model.parameters(),\n     #                         lr=learning_rate, weight_decay=weight_decay, momentum=momentum, foreach=True)\n    optimizer = optimizer = optim.Adam(model.parameters(),lr=learning_rate,weight_decay=weight_decay,betas=(momentum, 0.999), foreach=True)\n    IoU = IoUIndex()\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)  # goal: maximize Dice score\n    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n    criterion = nn.CrossEntropyLoss() if args.classes > 1 else DiceBCELoss()\n    global_step = 0\n\n    # 5. Begin training\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss, total_dice, total_tversky, total_iou = 0, 0, 0, 0\n        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img',position=0, leave = True,bar_format='{l_bar}{bar:10}{r_bar}{bar:-10}', colour='green') as pbar:\n            for i, batch in enumerate(train_loader):\n                images, true_masks = batch[0], batch[1]\n\n                assert images.shape[1] == args.n_channels, \\\n                    f'Network has been defined with {args.n_channels} input channels, ' \\\n                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n                    'the images are loaded correctly.'\n\n                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n                true_masks = true_masks.to(device=device, dtype=torch.long)\n\n                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n                    masks_pred = model(images)\n                    if args.classes == 1:\n                        loss = criterion(masks_pred.squeeze(1), true_masks.squeeze(1).float())\n                        #loss += dice_loss(F.sigmoid(masks_pred.squeeze(1)), true_masks.squeeze(1).float(), multiclass=False)\n                    else:\n                        loss = criterion(masks_pred, true_masks)\n                        loss += dice_loss(\n                            F.softmax(masks_pred, dim=1).float(),\n                            F.one_hot(true_masks, args.classes).permute(0, 3, 1, 2).float(),\n                            multiclass=True\n                        )\n\n\n                optimizer.zero_grad(set_to_none=True)\n                grad_scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n                grad_scaler.step(optimizer)\n                grad_scaler.update()\n                torch.cuda.empty_cache()\n                \n                pbar.update(images.shape[0])\n                global_step += 1\n                epoch_loss += loss.item()\n                #-------------------------------------------------------------------------------------------------\n                #-------------------------------------------------------------------------------------------------\n                # Calculate and accumulate Metrics\n                if args.classes == 1:\n                    predicted_masks = (F.sigmoid(masks_pred) > 0.5).float()\n                else:\n                    predicted_masks = F.softmax(masks_pred, dim=1).argmax(dim=1)\n\n                batch_dice = dice_coeff(predicted_masks, true_masks)\n                total_dice += batch_dice\n                # Calculate and accumulate Tversky coefficient\n                batch_tversky = tversky_coef(predicted_masks, true_masks)\n                total_tversky += batch_tversky\n                # Calculate and accumulate IoU coefficient\n                batch_iou = IoU(predicted_masks, true_masks)\n                total_iou += batch_iou\n                #-------------------------------------------------------------------------------------------------\n\n                pbar.set_postfix(**{'loss (batch)': loss.item(), f'Epoch {epoch} Dice': f'{batch_dice:.4f}', 'Tversky': f'{batch_tversky:.4f}', 'IoU': f'{batch_iou:.4f}'})\n\n                # Log after every 30 steps\n                if i % 30 == 0:\n                    run[npt_logger.base_namespace][\"batch/loss\"].append(loss.item())\n                    run[npt_logger.base_namespace][\"batch/batch dice\"].append(batch_dice)\n                    run[npt_logger.base_namespace][\"batch/batch IoU\"].append(batch_iou)\n                    run[npt_logger.base_namespace][\"batch/batch Tversky\"].append(batch_tversky)\n                \n        print(f'\\nEpoch {epoch} -Dice: {total_dice/num_train_batches:.4f} -Tversky: {total_tversky/num_train_batches:.4f} -IoU: {total_iou/num_train_batches:.4f}')\n        # Evaluation round\n\n        val_score = evaluate(args, model, val_loader, device, amp, criterion = criterion,dir_checkpoint =dir_checkpoint)\n        scheduler.step(val_score)\n        run[npt_logger.base_namespace][\"batch/Validation Dice Score\"].append(val_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:29.182015Z","iopub.execute_input":"2024-02-11T10:53:29.182381Z","iopub.status.idle":"2024-02-11T10:53:29.584364Z","shell.execute_reply.started":"2024-02-11T10:53:29.182352Z","shell.execute_reply":"2024-02-11T10:53:29.583559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')\n    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=1, help='Number of epochs')\n    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=16, help='Batch size')\n    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=1e-5,\n                        help='Learning rate', dest='lr')\n    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')\n    parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')\n    parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,\n                        help='Percent of the data that is used as validation (0-100)')\n    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')\n    parser.add_argument('--bilinear', action='store_true', default=False, help='Use bilinear upsampling')\n    parser.add_argument('--classes', '-c', type=int, default=1, help='Number of classes')\n    parser.add_argument('--n-channels', type=int, default=1, help='Number of input channels')\n\n    return parser.parse_args()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:29.585823Z","iopub.execute_input":"2024-02-11T10:53:29.586087Z","iopub.status.idle":"2024-02-11T10:53:29.595434Z","shell.execute_reply.started":"2024-02-11T10:53:29.586053Z","shell.execute_reply":"2024-02-11T10:53:29.594409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = get_args()\nargs.lr = .0001\nargs.load = None\nargs.bilinear = None","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:31.74143Z","iopub.execute_input":"2024-02-11T10:53:31.7418Z","iopub.status.idle":"2024-02-11T10:53:31.747572Z","shell.execute_reply.started":"2024-02-11T10:53:31.741769Z","shell.execute_reply":"2024-02-11T10:53:31.746624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    #model = Iternet(n_channels=1, n_classes=1, out_channels=32, iterations=2, transformer_config=TRANSCONFIG[\"ViT-B_32\"])\n    model = VisionTransformer(TRANSCONFIG['R50-ViT-B_16'], img_size=256, num_classes=1, zero_head=False, vis=True)\n    #model = Iternet( n_classes=1, out_channels=92, iterations=2, transformer_config=TRANSCONFIG[\"ViT-B_32\"])\n    #model = UNet_base(n_channels=3, n_classes=args.classes, bilinear=args.bilinear)\n    #model = Metapolyp(in_chans = 1)\n    model = model.to(memory_format=torch.channels_last)\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs for parallel processing.\")\n    # Use DataParallel to wrap the model\n\n    npt_logger = NeptuneLogger(\n        run=run,\n        model=model,\n        log_model_diagram=True,\n        log_gradients=False,\n        log_parameters=False,\n        log_freq=30,\n    )\n    model = nn.DataParallel(model)\n    if args.load:\n        state_dict = torch.load(args.load, map_location=device)\n        del state_dict['mask_values']\n        model.load_state_dict(state_dict)\n        logging.info(f'Model loaded from {args.load}')\n\n    model.to(device)\n\n    try:\n        train_model(\n            model=model,\n            args = args,\n            epochs=args.epochs,\n            batch_size=args.batch_size,\n            learning_rate=args.lr,\n            device=device,\n            img_scale=args.scale,\n            val_percent=args.val / 100,\n            amp=args.amp\n        )\n    except torch.cuda.OutOfMemoryError:\n        logging.error('Detected OutOfMemoryError! '\n                      'Enabling checkpointing to reduce memory usage, but this slows down training. '\n                      'Consider enabling AMP (--amp) for fast and memory efficient training')\n        torch.cuda.empty_cache()\n        model.use_checkpointing()\n        train_model(\n            model=model,\n            epochs=args.epochs,\n            batch_size=args.batch_size,\n            learning_rate=args.lr,\n            device=device,\n            img_scale=args.scale,\n            val_percent=args.val / 100,\n            amp=args.amp\n        )","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:53:32.032115Z","iopub.execute_input":"2024-02-11T10:53:32.032437Z","iopub.status.idle":"2024-02-11T10:53:38.680043Z","shell.execute_reply.started":"2024-02-11T10:53:32.03241Z","shell.execute_reply":"2024-02-11T10:53:38.678525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/checkpoints/best_checkpoint.pth","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = VisionTransformer(TRANSCONFIG['R50-ViT-B_16'], img_size=256, num_classes=1, zero_head=False, vis=True)\nargs.load = '/kaggle/working/checkpoints/best_checkpoint.pth'\nstate_dict = torch.load(args.load, map_location=device)\ndel state_dict['mask_values']\nmodel.load_state_dict(state_dict)\nlogging.info(f'Model loaded from {args.load}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set   = MedicalImageDataset('test',noise_typ=None)\nloader_args = dict(batch_size=1, num_workers=os.cpu_count(), pin_memory=True)\ntest_loader = DataLoader(test_set, shuffle=True, **loader_args)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss() if args.classes > 1 else DiceBCELoss()\ntest_score = evaluate(args, model.cuda(), test_loader, device, args.amp, criterion = criterion,dir_checkpoint =dir_checkpoint)","metadata":{},"execution_count":null,"outputs":[]}]}